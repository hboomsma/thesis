\chapter{Dead code elimination}
\label{ch:elimination}
% user
% vcs
% implications of wrongly deleted files

At this point we have identified code that possibly is dead. We know that used code is not dead, but we do not know whether unused code is not dead.
Before we can declare code dead, we should have an idea about the usage frequency of the code. For example the login page of a system is expected to be used every day, the invoicing system and related files are expected to be used once a month and VAT reports are only used once a year. Knowledge about how much a feature is expected to be used is very important in making the decision if files should be declared dead. 


\section{Decide which code to remove}

When cleaning an application, the first place to start is at the tree map (see figure \ref{fig:treemap}). The tree map shows per directory the number and the percentage of dead files. The size of each box corresponds to the number of dead files in that directory on a square root scale. The square root scale makes sure that directories with little or no dead files are still visible but at the same time spot which directories contain the most potentially dead files. The color of the boxes corresponds to the percentage of dead files contained in the directory. See \autoref{ch:visualization} for a more elaborate description of the tree map.

The tree map can be used to find files clustered together in a folder that are all dead. Files grouped together in a directory are usually related to each other. If a whole folder with related files is dead  it offers a starting point for removal. This method can also be used to detect unused plug-ins if they are located in the directory tree. An example can be seen in figure \ref{fig:aurora_modules}. The modules displayed are those of Aurora, one of the applications that we will look in to with the use case in \autoref{ch:evaluation}. Modules with only dead files can probably be removed if they are not expected to run with an frequency bigger than the period the usage data was collected in. Modules that are only partially dead require more inspection. Here the age of the files can help. If there are two files for almost the same thing and one is dead and the other is not and the dead one is not changed for a long time this could indicate that a feature was updated and the old file was not removed. For this case the graph that shows how many files are executed over time for the directory can be consulted. If the graph is stable for a long period and almost all files are accessed at the same time, it probably is less likely that the other files will be accessed in the future. The number of executions of the files in the folder also indicates the certainty because if it is very low it could very well be that not yet all possibilities and thus files are used.

\image{aurora_modules}{Aurora modules in treemap}{fig:aurora_modules}

The process of selecting code for elimination consists of first searching for the biggest related parts of an application that are marked as possible dead by the dead code identification process. When a coherent possibly dead part is found with no files that were executed we should check wherefore the code is used and if it would be expected to have run. If this is not the case it is possible to remove it. If the feature corresponding to the code at hand is not expected to be executed yet we could either wait for the feature to be used or access it by hand if this has no negative side effects for the operation of the organization. If we have removed all coherent parts of the application only containing dead files we can go on removing files from within these related parts. Here we look at the number of executions for the directory containing the files we want to remove. If this is very low or the graph with the number of executed files over time shows that new files are still executed regularly it is not a good idea to remove files from this directory unless we know that the file should not be used any more and this gets confirmed by the identification process. This process can be viewed in \autoref{fig:process}, the elimination part discussed in the next section is also visible in the figure.

\image{DeadCodeEliminationProcess}{The process of selecting code for elimination and removing it}{fig:process}

\section{Eliminating dead files}
After we have selected code for elimination we can not just remove the code and deploy the application. Before the code will be removed it will go trough a series of steps described in this section. 

When eliminating code the use of a \vcs is of hight importance because this allows to track back which files and methods has been removed and restore them if necessary. A decent process would let the engineer decide which files to remove and put that code change up for review. In this step the files are only removed in the personal system or branch in the \vcs. When the code review is approved, the change set will be committed  
to the current development branch. Now all engineers in the team have the code with the dead files removed. When everything is approved in integration and acceptance testing the change can be used in the next production version (see \autoref{fig:process}).

If code from a dead file is used by other code that code is dead too, otherwise the file would not have been marked dead. Functions that call a class defined in the dead file can also be (partially) removed and so on. The Eclipse \ide with \pdt or Zend Studio can help to find these dependencies, although the \dltk does not offer advanced type inference so it is hard to find all references but it is a start. For now we use full text search of the class and function names. This obviously only works for unique function names. A possible solution for this problem is the identification of dead methods which is described as future work in (\autoref{ch:future}).

When the code is unit tested it is a lot easier to be sure that there are no side effects of removing unreachable code. The code can be removed and if there are no failing tests everything should be fine. The problem lies in removing unused features. Unused features are accompanied by a unit test that also should be removed. 

If continuous integration\cite{fowler2006} is used, it is best to use small commits to the \vcs. This way it is more easy to find which files broke the unit tests. When continuous integration is used defects due to the elimination of files can be detected and handled early. Hostnet uses continuous integration for all projects. 

\section{Effect of wrongly removed code} 

\image{500}{HTTP 500 error page in the webshop}{fig:500}

Despite the effort to only remove code that will never be called upon again it is possible that some used code gets removed. When PHP tries to load a file that is not there it will stop execution with a fatal error and emit a HTTP 500 - Internal Server Error, as shown in \autoref{fig:500}. These error pages are logged by PHP and it is possible to customize an error page so the call stack will be e-mailed to the developers. 

If an engineer receives an email he can remove the part of code calling upon the deleted file or place it back from the source repository. It is worth noting that because it concerns a web application, not the whole application crashes and the user will not lose valuable work because the file was removed. At Hostnet we sent an e-mail report for every internal server error to the developers.

\section{Ceteris paribus not possible}

It is common when taking measurements to only vary one input at a time and keep all other things the same so the effect of that one input can be measured. This is called ceteris paribus, keeping all other things the same. Because the measurements take place in a live application it is not possible to freeze it from updates for at least several months to acquire the right measurements in a undisturbed situation. Especially in the fast moving web world this is impossible. Therefore we have to cope with the fact that there are updates coming in all the time. This implies that unused code is not always dead. It could be that it is just not used yet or that it is just added to the application but not enabled for the end-user yet. To be able to detect this last type it is possible to take \vcs data into account. At Hostnet almost all projects are stored in \svn and some bleeding edge ones in git. Using this data it is possible to detect that a file is recently added to the system and therefore should not be declared dead. The date a file was last change in the \vcs is displayed in the age column shown in \autoref{fig:treemap} as described in \autoref{ch:visualization}. To get a better view on the certainty that new features will be accessed for a directory it is possible to look at the graph (also in \autoref{fig:treemap}) which shows new files accessed over time. If all new files added to an application would also be displayed in this graph it would not give a good view of the certainty because files for newly added feature would be included making it look that it takes longer to stabilize then is the case in reality. By not adding files to the graph that has been added later on this problem is prevented. The files will still be displayed in the tree map for a complete overview of the application.