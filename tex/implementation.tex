\chapter{Implementation of dead code identification}
\label{ch:implementation}

In this chapter the toolbox that is created to be able to perform the case studies is discussed. Before it is possible to look at the implementation directly the differences and properties of the web applications under inspection are looked upon. The visualization of the data can be found in the next chapter (chapter \ref{ch:visualization}).

\section{Web applications in PHP}

The main differences between a web application written in PHP and a conventional application is that the web application is in memory for a very short period and a conventional application resides in memory for the time the users uses it. This means that the application is only active when the user clicks a link or submits data. This could go via a normal HTTP request or through AJAX\cite{garrett2005}. In both cases the browser sends a request to web server which will spawn a PHP process to handle the request and return the result to the browser. When a PHP process crashes, the web page shown to the user will be an 500 error page but the application as a whole will still function. Only just submitted data will be lost, and in most modern browsers pushing the back button will preserve even that data.

Due to the short lived nature of web applications it is possible to just store the aggregated data at the end of every execution. If a page crashes no valuable statistical data will be lost; code that is used by a page that crashes could be interesting to have for debugging purposes but it is of no value for the code in use because the feature clearly does not work. It is worth noting that \furl{zendserver} has functionality to save a full code trace of a crashed page. This could also be done for every page but this will give far too much data to progress because the API does not offer usable aggregation or coverage options. This has to be done afterwards. For a small application this could be possible but for a busy site this will use up the memory.

In a web application the overhead can be measured in the amount of time extra used per page load. The overhead relative to the total length of the page load is not of much importance. Only the time that is extra used for a page is important because if the overhead is too big it will be noticed by the user. For example if a page is very quick and does not have any database interaction the overhead will be really big relative to the total length of execution, but the end user would still not notice this because the load time is still short enough to go unnoticed. This is only true for short lived scripts showing direct output to the user, for a long(er) lived batch job which can be found in the provision system and which are used to create the invoices. For these scripts the overhead relative to the total execution time is important because the periodic tasks could take too long with the dynamic analysis enabled. The goal is to have the overhead go unnoticed by the end users.

\section{Dynamic alive file identification in PHP}
\label{sec:alive}
% - why dead file
% - why this method and not relying on Xdebug or Zend
% - hydrating the database
% - accumulating data
%\image{zend-server}{Zend Server web interface with the auto\_append\_file directive}{fig:zendserver}

PHP offers functionality to retrieve all included files from a running process. To know all files used, this function should be called after the normal execution has finished. Because this function loads all files that are included and does not know if those files are actually used this only provided used files if they are dynamically loaded and are not statically included.  The function call \verb|array get_included_files(void)| gives an array with all files.


Since version 5, PHP has the ability to automatically load classes. This feature is also used by \furl{symfony} the framework used by Hostnet\cite{potencier2010}; it is also used by almost any other framework for PHP. With PHP it is possible to create a function that will be called when a unknown class is instantiated. That function may then take care of loading a file containing the class definition so it will become available before the execution is continued. If the auto load function did not load the needed class definition for the unknown class PHP will fail with an error\cite{php}.

Now we have the method that will give a list of all used files, but those file names still have to be saved in a central database. This has to be done after all normal execution. To accomplish this we could add the call to the \verb|*.php| files that are accessed by the users and web server but PHP offers the possibility to automatically prepend and append files to each and every PHP execution no matter if it is executed through the \cli, \cgi or as Apache module. This property can be set in the  \verb|php.ini|, the global configuration file of PHP, with the  \verb|auto_append_file| directive\furl*{append} This option can also be set via a \verb|.htaccess| file which controls access properties for underlying folders in the Apache web server. This could prove useful to only turn on the analysis for the web server or for some specific directories. At Hostnet we set this property system wide via the \verb|php.ini|. This has the effect that the setting is applied machine wide. So even batch jobs that are started by hand will be taken into account. All logic that sends the used files that have been measured for a request to the central database can be put in to a single file. In this implementation this file is called \verb|append.php| (see \nameref{ch:append.php} for an example implementation). This file is then set to be automatically be appended to every execution of PHP. This files also takes care of the aggregation of the data. The dates in the database about when the file was last used and first used are updated when needed and the usage counter is increased.

MySQL is used as storage solution because it was already available as service at Hostnet and is a very common companion of PHP and Apache. A central MySQL server eases the handling of multiple servers and allows to run all visualization on other server without file transfers. The current implemented solution makes uses of \pdo so the toolbox can easily be integrated with other storage solutions like \furl{sqlite} which does not requires any extra software or drivers to be installed or set-up. This safes the user the trouble of setting up a MySQL server for the sole purpose of analysing one application.

To add files to the database a simple listing is done of the directory structure is used. Only \verb|.php| files are added to the database. When an application is deployed to the production server a new listing is done to be able to remove all deleted files from the database. Files that are not longer in the application will not actually be removed from the databased but only tagged with the date when the removal was detected because with this historic date it is possible to visualize the state of the system in any moment in time since the start of measuring. Every application is primed when a new version of the application is deployed. The deployment of an new application is as simple as an automated \svn checkout on the production server and a new symbolic link to the right folder. After the checkout some (post) configuration is done by the \furl{makefile} in the project. This way other configuration files are loaded for the development, testing, acceptance, staging and production environment are loaded. Via the deployment  Make file the \verb|prime| subcommand of the toolbox (see appendix \ref{sec:prime}) is called.

For every automated deployment system it will suffice to add a call to the \verb|prime| subcommand of the toolbox to the process. If there is no automated process this command should be ran by hand every time new files are added or removed from an application when it is deployed.

As described in the previous \hyperref[ch:identification]{chapter} also \vcs data should be added to the database. This is implemented in the \verb|prime| subcommand that is also used to add the file names to the database as option. Using the \verb|--vcs svn| flag the command will also parse the \svn data of the files. Other \glspl{vcs} can easily be added, most of the work for Git has already been done.

\subsection*{Overview of the implemented tools}
\image{ToolChain}{overview of the dead code identification process}{fig:toolchain}

An overview of the process can be viewed in \autoref{fig:toolchain}. All available files are loaded from disk together with the \vcs data if needed and available. This information is stored in a database and refreshed when a new version of the application is deployed. This database is updated by the \verb|append.php| file adding information about when the file is last used and how many times it is used. The aggregation by \verb|append.php| is done on a file level. From this database containing all raw data it is possible to create graphs showing how the number of used files changes over time, but this data can also be aggregated over the directory structure to provide the percentage of dead code per directory and file. This information can be used for the plug-in. This aggregated data is also used for the tree map visualization, for this visualization the data is cached in an separate database. The tree map visualization also includes the graph. More on these visualizations can be found in the next \hyperref[ch:visualization]{chapter}.