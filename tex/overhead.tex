\section{Overhead}
\label{sec:overhead}

The overhead caused by reporting all used files to the central database should be low enough to not be noticed by the end user of the application. At the end of each request from the browser for a new page, a list of used files is sent to the central database. Prior to building up the connection and sending the list, the response from the web application is flushed to the browser. This way the browser will already render the web page and only indicates that the page is still loading. This is convenient because even when the overhead is noticeable the end user can still access the application without delay.

We look at the overhead in time per page request. Profiling code has been added to the analysis in Aurora to measure the overhead. Aurora is the most used application within Hostnet and has the biggest number of files. The overhead in Aurora should be the worst case within Hostnet, as the time to send the list to the database and process it, is dependent on the size of the list and the load on the database server. Concurrent write operations to a single MySQL data base on the same row will lock and create bigger load times when the load on the server increases. It is very likely that the same row will be updated in every request to the database because every request to Aurora will pass through the \verb|index.php| file which is the starting point for the application.

When measuring the overhead, the time spent on the analysis is measured as well as the time required to build a connection to the database to be able to see if network congestion could be a limiting factor. A plot of this overhead can be viewed in \autoref{fig:overhead}. On the x-axis the time spent in milliseconds is shown. On the left y-axis the frequency of the corresponding amount of overhead is displayed. In red the overhead for the creation of the connection to the MySQL database on the database server is plotted. The time needed to build the connection has an average at 1.6~ms with and standard deviation of 0.6~ms. The connection overhead seems normally distributed with a standard deviation of under 1~ms and without additional peaks which indicates there is no congestion on the network. In green the frequencies of total overhead have been plotted. The average overhead is 7.8~ms with an standard deviation of 153.5~ms. The blue curve denotes the probability that the overhead will be less than the corresponding value on the x-axis. Here we can see that in over 95\% of the measured cases the overhead stays below 6~ms.

\image[.9\textwidth]{overhead}{Overhead caused by measuring included files (in memory)}{fig:overhead}

When we use a slower method of storage, for example on disk storage in the database server we notice that two peaks will appear in the total overhead curve (see \autoref{fig:overheaddisk}). These appear because if the table is locked the query will have to wait a few milliseconds. Using disk storage, the average overhead is 27.5~ms with an standard deviation of 231~ms. Still in 80\% of the cases the overhead is below 10~ms. In 95\% of the cases the overhead is below 23~ms. The problem of locking rows is clearly visible here. The use of a memory table will not solve this problem but helps us determine where the peaks came from. For the analysis at Hostnet the slower on disk storage is used so that there is no need to build a backup mechanism to make sure that when the database server would be restarted the data would not be lost. 

\image[.9\textwidth]{overhead_disk}{Overhead caused by measuring included files (on disk)}{fig:overheaddisk}

When the load on the database becomes too high other solutions must be found to keep the overhead within bounds so it will not be noticed. One solution could be to use multiple MySQL servers and aggregate the date between them less frequently then the normal updates occur. An other solution could be to aggregate the data on the web server side in shared memory and send it to the server at a frequency less then that of the page requests.

From the experience at Hostnet we can say that the overhead was not noticed by the helpdesk and account managers using Aurora for every day work even when using disk storage. We found that the limiting factor is not the project size but the frequency of requests because locks in the database will occur that create a bigger overhead than the time needed to send the list of files to the database.

